# 動作空間設計文檔

## 概述

本文檔詳細說明 Agent 的動作空間設計，包含離散動作、連續瞄準 Actor、以及與技能系統的整合。

---

## 動作空間總覽

### 離散動作（11 個選項）

| ID | 動作 | 說明 |
|----|------|------|
| 0 | MOVE_FORWARD | 向前移動 |
| 1 | TURN_LEFT | 左轉 |
| 2 | TURN_RIGHT | 右轉 |
| 3 | SKILL_OUTER_SLASH | 外圈刮 |
| 4 | SKILL_MISSILE | 飛彈 |
| 5 | SKILL_HAMMER | 鐵錘 |
| 6 | SKILL_DASH | 閃現 |
| 7 | SKILL_SOUL_CLAW | 靈魂爪 |
| 8 | SKILL_SOUL_PALM | 靈魂掌 |
| 9 | SKILL_BLOOD_POOL | 血池 |
| 10 | SKILL_SUMMON_PACK | 召喚血包 |

### 連續 Actor（6 個）

| ID | Actor 名稱 | 對應技能 | 輸出範圍 | 說明 |
|----|-----------|---------|---------|------|
| 0 | aim_missile | 飛彈 | [-π, π] | 發射角度偏移 |
| 1 | aim_hammer | 鐵錘 | [-π, π] | 錘擊角度偏移 |
| 2 | aim_dash_direction | 閃現 | [-π, π] | 閃現方向 |
| 3 | aim_dash_facing | 閃現 | [-π, π] | 閃現後朝向 |
| 4 | aim_claw | 靈魂爪 | [-π, π] | 爪擊角度偏移 |
| 5 | aim_palm | 靈魂掌 | [-π, π] | 掌擊角度偏移 |

### 技能與 Actor 的映射

```python
SKILL_TO_ACTORS = {
    3: [],           # 外圈刮 - 無需瞄準
    4: [0],          # 飛彈 - aim_missile
    5: [1],          # 鐵錘 - aim_hammer
    6: [2, 3],       # 閃現 - aim_dash_direction, aim_dash_facing
    7: [4],          # 靈魂爪 - aim_claw
    8: [5],          # 靈魂掌 - aim_palm
    9: [],           # 血池 - 無需瞄準
    10: [],          # 召喚血包 - 無需瞄準
}
```

---

## 程式碼架構

### Agent 類別修改

```python
# ai/agent.py

class HybridPPOAgent:
    """
    混合動作空間 PPO Agent

    使用 squared probability distribution 而非 softmax
    """

    # 動作空間定義
    N_DISCRETE_ACTIONS = 11
    N_AIM_ACTORS = 6

    # 動作枚舉
    MOVE_FORWARD = 0
    TURN_LEFT = 1
    TURN_RIGHT = 2
    SKILL_OUTER_SLASH = 3
    SKILL_MISSILE = 4
    SKILL_HAMMER = 5
    SKILL_DASH = 6
    SKILL_SOUL_CLAW = 7
    SKILL_SOUL_PALM = 8
    SKILL_BLOOD_POOL = 9
    SKILL_SUMMON_PACK = 10

    # 技能到 Actor 的映射
    SKILL_TO_ACTORS = {
        3: [],
        4: [0],
        5: [1],
        6: [2, 3],
        7: [4],
        8: [5],
        9: [],
        10: [],
    }

    def __init__(
        self,
        n_features: int,
        gamma: float = 0.99,
        lmbda: float = 0.95,
        epsilon: float = 0.2,
        sigma_init: float = 0.6,
        sigma_min: float = 0.15,
        sigma_decay: float = 0.9998,
        lr_actor_discrete: float = 0.003,
        lr_actor_continuous: float = 0.002,
        lr_critic: float = 0.007
    ):
        self.n_features = n_features
        self.gamma = gamma
        self.lmbda = lmbda
        self.epsilon = epsilon

        # 連續動作的探索標準差
        self.sigma = sigma_init
        self.sigma_min = sigma_min
        self.sigma_decay = sigma_decay

        # 學習率
        self.lr_actor_discrete = lr_actor_discrete
        self.lr_actor_continuous = lr_actor_continuous
        self.lr_critic = lr_critic

        # 初始化權重
        self._init_weights()

    def _init_weights(self):
        """初始化網路權重"""
        # 離散 Actor: (11, n_features)
        self.w_actor_discrete = np.random.randn(
            self.N_DISCRETE_ACTIONS, self.n_features
        ) * 0.01

        # 連續 Actors: 每個 Actor 有自己的權重 (n_features,)
        self.w_aim_actors = [
            np.random.randn(self.n_features) * 0.01
            for _ in range(self.N_AIM_ACTORS)
        ]

        # Critic: (n_features,)
        self.w_critic = np.random.randn(self.n_features) * 0.01

    # ==================== 動作選擇 ====================

    def select_action(
        self,
        features: np.ndarray,
        skill_available: np.ndarray = None
    ) -> Tuple[int, List[float]]:
        """
        選擇動作

        Args:
            features: 特徵向量 (n_features,)
            skill_available: 技能可用性遮罩 (8,)，True 表示可用

        Returns:
            (discrete_action, aim_values)
            - discrete_action: 0-10 的離散動作
            - aim_values: 對應技能的瞄準值列表（如果不需要瞄準則為空列表）
        """
        # 計算離散動作概率
        probs = self._compute_discrete_probs(features, skill_available)

        # 採樣離散動作
        discrete_action = np.random.choice(self.N_DISCRETE_ACTIONS, p=probs)

        # 如果是技能動作，計算瞄準值
        aim_values = []
        if discrete_action >= 3:  # 是技能
            actor_indices = self.SKILL_TO_ACTORS.get(discrete_action, [])
            for actor_idx in actor_indices:
                aim_value = self._compute_aim_value(features, actor_idx)
                aim_values.append(aim_value)

        return discrete_action, aim_values

    def _compute_discrete_probs(
        self,
        features: np.ndarray,
        skill_available: np.ndarray = None
    ) -> np.ndarray:
        """
        計算離散動作概率（使用 squared probability）

        P_i = logit_i² / Σ(logit_k²)
        """
        # 計算 logits
        logits = self.w_actor_discrete @ features  # (11,)

        # 應用技能可用性遮罩
        if skill_available is not None:
            # skill_available 是長度 8 的陣列，對應 SKILL_1 到 SKILL_8
            for i, available in enumerate(skill_available):
                if not available:
                    logits[3 + i] = -1e10  # 遮罩掉不可用的技能

        # Squared probability
        squared = logits ** 2
        probs = squared / (squared.sum() + 1e-8)

        return probs

    def _compute_aim_value(self, features: np.ndarray, actor_idx: int) -> float:
        """
        計算瞄準值（連續動作）

        使用高斯分佈採樣
        """
        # 計算均值
        mu = self.w_aim_actors[actor_idx] @ features

        # 加入探索噪聲
        noise = np.random.randn() * self.sigma
        aim_value = mu + noise

        # 限制在 [-π, π] 範圍
        aim_value = np.clip(aim_value, -np.pi, np.pi)

        return aim_value

    # ==================== 價值估計 ====================

    def estimate_value(self, features: np.ndarray) -> float:
        """估計狀態價值"""
        return self.w_critic @ features

    # ==================== 訓練 ====================

    def update(self, trajectory: List[Tuple]):
        """
        PPO 更新

        Args:
            trajectory: [(state, discrete_action, aim_values, reward, next_state, done), ...]
        """
        # 計算 GAE
        advantages, returns = self._compute_gae(trajectory)

        # 更新 Critic
        self._update_critic(trajectory, returns)

        # 更新離散 Actor
        self._update_discrete_actor(trajectory, advantages)

        # 更新連續 Actors
        self._update_continuous_actors(trajectory, advantages)

        # 衰減探索標準差
        self.sigma = max(self.sigma * self.sigma_decay, self.sigma_min)

    def _compute_gae(self, trajectory):
        """計算 Generalized Advantage Estimation"""
        T = len(trajectory)
        advantages = np.zeros(T)
        returns = np.zeros(T)

        # 反向計算
        last_gae = 0
        for t in reversed(range(T)):
            state, _, _, reward, next_state, done = trajectory[t]

            if done:
                next_value = 0
            else:
                next_value = self.estimate_value(next_state)

            current_value = self.estimate_value(state)
            delta = reward + self.gamma * next_value - current_value

            advantages[t] = last_gae = delta + self.gamma * self.lmbda * last_gae * (1 - done)
            returns[t] = advantages[t] + current_value

        return advantages, returns

    def _update_critic(self, trajectory, returns):
        """更新 Critic 權重"""
        for t, (state, _, _, _, _, _) in enumerate(trajectory):
            value = self.estimate_value(state)
            error = returns[t] - value

            # 梯度更新
            self.w_critic += self.lr_critic * error * state

    def _update_discrete_actor(self, trajectory, advantages):
        """更新離散 Actor 權重"""
        for t, (state, action, _, _, _, _) in enumerate(trajectory):
            probs = self._compute_discrete_probs(state)

            # Policy gradient with baseline
            for a in range(self.N_DISCRETE_ACTIONS):
                if a == action:
                    grad = (1 - probs[a]) * state
                else:
                    grad = -probs[a] * state

                self.w_actor_discrete[a] += self.lr_actor_discrete * advantages[t] * grad

    def _update_continuous_actors(self, trajectory, advantages):
        """更新連續 Actors 權重"""
        for t, (state, discrete_action, aim_values, _, _, _) in enumerate(trajectory):
            if discrete_action < 3:  # 不是技能
                continue

            actor_indices = self.SKILL_TO_ACTORS.get(discrete_action, [])

            for i, actor_idx in enumerate(actor_indices):
                if i >= len(aim_values):
                    continue

                aim_value = aim_values[i]
                mu = self.w_aim_actors[actor_idx] @ state

                # Gaussian policy gradient
                # ∂log π / ∂μ = (a - μ) / σ²
                grad_mu = (aim_value - mu) / (self.sigma ** 2 + 1e-8)

                # ∂μ / ∂w = state
                self.w_aim_actors[actor_idx] += (
                    self.lr_actor_continuous * advantages[t] * grad_mu * state
                )
```

---

## 動作執行流程

```python
# game/world.py

class GameWorld:
    def execute_action(
        self,
        discrete_action: int,
        aim_values: List[float]
    ) -> str:
        """
        執行 Agent 動作

        Args:
            discrete_action: 0-10 的離散動作
            aim_values: 瞄準值列表

        Returns:
            事件描述字串
        """
        if self.player is None:
            return ""

        # 移動/轉向動作
        if discrete_action == 0:  # MOVE_FORWARD
            success = self.physics.move_forward(self.player, speed=0.6)
            return "HIT WALL!" if not success else ""

        elif discrete_action == 1:  # TURN_LEFT
            self.physics.rotate_entity(self.player, 0.4)
            return ""

        elif discrete_action == 2:  # TURN_RIGHT
            self.physics.rotate_entity(self.player, -0.4)
            return ""

        # 技能動作
        else:
            skill_id = discrete_action - 2  # 轉換為技能 ID (1-8)
            return self._execute_skill(skill_id, aim_values)

    def _execute_skill(self, skill_id: int, aim_values: List[float]) -> str:
        """執行技能"""
        # 檢查技能是否可用
        if not self.skill_executor.can_cast(skill_id):
            return "SKILL NOT READY"

        # 開始施放
        success = self.skill_executor.start_cast(skill_id, aim_values)

        if success:
            skill = self.skill_registry.get(skill_id)
            return f"CASTING {skill.name}..."
        else:
            return "CAST FAILED"
```

---

## 技能可用性遮罩

在訓練時，需要告訴 Agent 哪些技能當前可用：

```python
def get_skill_availability(self, world: GameWorld) -> np.ndarray:
    """
    獲取技能可用性遮罩

    Returns:
        (8,) 的布林陣列，True 表示技能可用
    """
    availability = np.zeros(8, dtype=bool)

    cd_manager = world.skill_executor.cooldown_manager

    for skill_id in range(1, 9):
        availability[skill_id - 1] = cd_manager.is_ready(skill_id)

    return availability
```

---

## 權重導出格式

```python
def export_weights(self) -> dict:
    """導出權重為 JSON 相容格式"""
    return {
        "w_actor_discrete": self.w_actor_discrete.tolist(),
        "w_aim_actors": [w.tolist() for w in self.w_aim_actors],
        "w_critic": self.w_critic.tolist(),
        "sigma": self.sigma
    }

def load_weights(self, data: dict):
    """從 JSON 載入權重"""
    self.w_actor_discrete = np.array(data["w_actor_discrete"])
    self.w_aim_actors = [np.array(w) for w in data["w_aim_actors"]]
    self.w_critic = np.array(data["w_critic"])
    self.sigma = data.get("sigma", self.sigma_min)
```

---

## 施法期間的動作限制

某些技能施放期間可能限制移動：

```python
def select_action(self, features, skill_available, is_casting, can_move_while_casting):
    """
    考慮施法狀態的動作選擇
    """
    if is_casting:
        if can_move_while_casting:
            # 只能選擇移動動作
            # 遮罩所有技能
            skill_available = np.zeros(8, dtype=bool)
        else:
            # 完全不能動作，返回空操作
            return -1, []  # 或其他表示「等待」的特殊值

    return self._select_action_internal(features, skill_available)
```

---

## 測試方法

```python
def test_action_selection():
    agent = HybridPPOAgent(n_features=98)

    # 隨機特徵
    features = np.random.randn(98)
    features[-1] = 1.0  # Bias

    # 所有技能可用
    skill_available = np.ones(8, dtype=bool)

    # 選擇動作
    discrete, aim_values = agent.select_action(features, skill_available)

    assert 0 <= discrete <= 10

    # 檢查 aim_values 長度
    if discrete == 6:  # 閃現
        assert len(aim_values) == 2
    elif discrete in [4, 5, 7, 8]:  # 需要瞄準的技能
        assert len(aim_values) == 1
    else:
        assert len(aim_values) == 0

def test_skill_masking():
    agent = HybridPPOAgent(n_features=98)
    features = np.random.randn(98)

    # 只有前 3 個技能可用
    skill_available = np.array([True, True, True, False, False, False, False, False])

    # 多次採樣，確保不會選到不可用的技能
    for _ in range(100):
        discrete, _ = agent.select_action(features, skill_available)
        if discrete >= 3:
            assert discrete <= 5  # 只能是技能 1, 2, 3
```
